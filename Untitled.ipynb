{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc47bbf5-28dd-41d4-9ce9-425e72055ffa",
   "metadata": {},
   "source": [
    "# The RAG Development model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbce22f-a42f-41ad-9dda-bfa5b67d0f1a",
   "metadata": {},
   "source": [
    "### Pre_Steps : installation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56eb1ae4-9aaf-4f4d-9146-c648c9db18a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from PyPDF2) (4.9.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/sushildube/Desktop/ML_projects/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: transformers in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (4.38.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: requests in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: filelock in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/sushildube/Desktop/ML_projects/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: sympy in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: fsspec in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: networkx in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: filelock in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sushildube/Desktop/ML_projects/env/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/sushildube/Desktop/ML_projects/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pytorch\n",
      "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Users/sushildube/Desktop/ML_projects/env/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-wheel-4_5tbxb7\n",
      "       cwd: /private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/\n",
      "  Complete output (5 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/setup.py\", line 15, in <module>\n",
      "      raise Exception(message)\n",
      "  Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "Installing collected packages: pytorch\n",
      "    Running setup.py install for pytorch ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/sushildube/Desktop/ML_projects/env/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-record-d_bmiq5p/install-record.txt --single-version-externally-managed --compile --install-headers /Users/sushildube/Desktop/ML_projects/env/include/site/python3.9/pytorch\n",
      "         cwd: /private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/\n",
      "    Complete output (5 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/setup.py\", line 11, in <module>\n",
      "        raise Exception(message)\n",
      "    Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /Users/sushildube/Desktop/ML_projects/env/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-install-uervr0fw/pytorch_3846037d514947749348babdd10bb0b0/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/fy/r87rd9vs6wv45d53zjpnhfxr0000gn/T/pip-record-d_bmiq5p/install-record.txt --single-version-externally-managed --compile --install-headers /Users/sushildube/Desktop/ML_projects/env/include/site/python3.9/pytorch Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/sushildube/Desktop/ML_projects/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82801d2d-4861-401a-8c7f-290a2dac3c53",
   "metadata": {},
   "source": [
    "### Step 1: Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "591eac66-f4a9-4a58-b13a-ed371a58e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AdamWeightDecay\n",
    "from PyPDF2 import PdfReader\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ec3f550-5d71-4354-b7c9-9989709b46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CustomDataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_text': self.texts[idx], 'target_text': self.labels[idx]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35b173-f427-4aa5-a837-391e2b7ef5cf",
   "metadata": {},
   "source": [
    "### Step 2: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12323d05-7bc8-4bbb-8304-e229ed65fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_number].extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce1906-b270-4b47-8de9-6903ee63a428",
   "metadata": {},
   "source": [
    "### Load PDFs from a designated folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d50f33ce-5de8-4ae9-a53f-28a54ac5e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = \"pdfs_train/\"\n",
    "pdf_files = [os.path.join(pdf_folder, file) for file in os.listdir(pdf_folder) if file.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d98c71c5-9b49-41e3-89e0-0f1743bcd034",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [extract_text_from_pdf(pdf_file) for pdf_file in pdf_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877e2c9-188c-4e7f-ba25-3884b3d9d2c8",
   "metadata": {},
   "source": [
    "### Step 2: Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41b1dc83-deb1-4e08-83ca-0f0a26fbc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd0aad-5dc5-40ef-915b-084abf4e57c8",
   "metadata": {},
   "source": [
    "### Step 3: Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "87a4bee3-f62e-44b0-ae9b-d58fe166fe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "generation_model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc39c296-54c9-4713-942b-b239e40a3546",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'tfgpt2lm_head_model_2' (type TFGPT2LMHeadModel).\n\n`labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(1, 3) and logits.shape=(1, 511, 50257)\n\nCall arguments received by layer 'tfgpt2lm_head_model_2' (type TFGPT2LMHeadModel):\n  • input_ids={'input_ids': 'tf.Tensor(shape=(1, 512), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(1, 512), dtype=int32)'}\n  • past_key_values=None\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=tf.Tensor(shape=(1, 4), dtype=int32)\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m tokenizer(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m'\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgeneration_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     15\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, generation_model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m~/Desktop/ML_projects/env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/ML_projects/env/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:428\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    427\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_projects/env/lib/python3.9/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:955\u001b[0m, in \u001b[0;36mTFGPT2LMHeadModel.call\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m    953\u001b[0m     shifted_logits \u001b[38;5;241m=\u001b[39m logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    954\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 955\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshifted_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    958\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m transformer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/Desktop/ML_projects/env/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:225\u001b[0m, in \u001b[0;36mTFCausalLanguageModelingLoss.hf_compute_loss\u001b[0;34m(self, labels, logits)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_fn(labels, reduced_logits)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Clip negative labels to zero here to avoid NaNs and errors - those positions will get masked later anyway\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m unmasked_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# make sure only labels that are not equal to -100 affect the loss\u001b[39;00m\n\u001b[1;32m    227\u001b[0m loss_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, dtype\u001b[38;5;241m=\u001b[39munmasked_loss\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tfgpt2lm_head_model_2' (type TFGPT2LMHeadModel).\n\n`labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(1, 3) and logits.shape=(1, 511, 50257)\n\nCall arguments received by layer 'tfgpt2lm_head_model_2' (type TFGPT2LMHeadModel):\n  • input_ids={'input_ids': 'tf.Tensor(shape=(1, 512), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(1, 512), dtype=int32)'}\n  • past_key_values=None\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=tf.Tensor(shape=(1, 4), dtype=int32)\n  • training=False"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        inputs = tokenizer(batch['input_text'], return_tensors='tf', max_length=512, truncation=True)\n",
    "        labels = tokenizer(batch['target_text'], return_tensors='tf', max_length=512, truncation=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = generation_model(inputs, labels=labels['input_ids'])\n",
    "            loss = outputs.loss\n",
    "\n",
    "        gradients = tape.gradient(loss, generation_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, generation_model.trainable_variables))\n",
    "\n",
    "        total_loss += loss.numpy()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48eeb2-02a3-46ff-82ec-f9156e5ded72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder labels for training, replace with your actual labels\n",
    "labels = [\"context text for retrieval\", \"generation target text\"]\n",
    "\n",
    "# Create a custom dataset\n",
    "train_dataset = CustomDataset(corpus, labels)\n",
    "\n",
    "# DataLoader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Fine-tune the model (placeholder - actual fine-tuning depends on your task)\n",
    "optimizer = AdamW(generation_model.trainable_variables, learning_rate=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55027857-54ad-47b4-b647-acf8b051055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "generation_model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "417e5a92-e8b6-4c43-819c-23055d310b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder labels for training, replace with your actual labels\n",
    "labels = [\"context text for retrieval\", \"generation target text\"]\n",
    "\n",
    "# Create a custom dataset\n",
    "train_dataset = CustomDataset(corpus, labels)\n",
    "\n",
    "# DataLoader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Fine-tune the model (placeholder - actual fine-tuning depends on your task)\n",
    "optimizer = AdamWeightDecay(learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c3b1054c-a981-4608-9a60-0255b33a4c6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'tfgpt2lm_head_model_3' (type TFGPT2LMHeadModel).\n\n`labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(1, 3) and logits.shape=(1, 511, 50257)\n\nCall arguments received by layer 'tfgpt2lm_head_model_3' (type TFGPT2LMHeadModel):\n  • input_ids={'input_ids': 'tf.Tensor(shape=(1, 512), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(1, 512), dtype=int32)'}\n  • past_key_values=None\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=tf.Tensor(shape=(1, 4), dtype=int32)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m tokenizer(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m'\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgeneration_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Set training=True during training\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     13\u001b[0m trainable_variables \u001b[38;5;241m=\u001b[39m generation_model\u001b[38;5;241m.\u001b[39mtrainable_variables\n",
      "File \u001b[0;32m~/Desktop/ML_projects/env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/ML_projects/env/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:428\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    427\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_projects/env/lib/python3.9/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:955\u001b[0m, in \u001b[0;36mTFGPT2LMHeadModel.call\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m    953\u001b[0m     shifted_logits \u001b[38;5;241m=\u001b[39m logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    954\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 955\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshifted_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    958\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m transformer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/Desktop/ML_projects/env/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:225\u001b[0m, in \u001b[0;36mTFCausalLanguageModelingLoss.hf_compute_loss\u001b[0;34m(self, labels, logits)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_fn(labels, reduced_logits)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Clip negative labels to zero here to avoid NaNs and errors - those positions will get masked later anyway\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m unmasked_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# make sure only labels that are not equal to -100 affect the loss\u001b[39;00m\n\u001b[1;32m    227\u001b[0m loss_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, dtype\u001b[38;5;241m=\u001b[39munmasked_loss\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tfgpt2lm_head_model_3' (type TFGPT2LMHeadModel).\n\n`labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(1, 3) and logits.shape=(1, 511, 50257)\n\nCall arguments received by layer 'tfgpt2lm_head_model_3' (type TFGPT2LMHeadModel):\n  • input_ids={'input_ids': 'tf.Tensor(shape=(1, 512), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(1, 512), dtype=int32)'}\n  • past_key_values=None\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=tf.Tensor(shape=(1, 4), dtype=int32)\n  • training=True"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        inputs = tokenizer(batch['input_text'], return_tensors='tf', max_length=512, truncation=True)\n",
    "        labels = tokenizer(batch['target_text'], return_tensors='tf', max_length=512, truncation=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = generation_model(inputs, labels=labels['input_ids'], training=True)  # Set training=True during training\n",
    "            loss = outputs.loss\n",
    "\n",
    "        trainable_variables = generation_model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        total_loss += loss.numpy()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "73062354-2062-4497-b29f-09f213068516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_folder = \"pdf_results\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_model_path = os.path.join(output_folder, \"fine_tuned_model\")\n",
    "generation_model.save_pretrained(output_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5e432007-56fa-4f9d-8b5b-de66e1a3cb34",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: './pdf_results/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Example usage for updating and retraining\u001b[39;00m\n\u001b[1;32m     17\u001b[0m new_pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pdf_results/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mupdate_and_retrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 3\u001b[0m, in \u001b[0;36mupdate_and_retrain\u001b[0;34m(new_pdf_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_and_retrain\u001b[39m(new_pdf_path):\n\u001b[0;32m----> 3\u001b[0m     new_text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Update the retrieval model's index\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     tfidf_matrix \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform(corpus \u001b[38;5;241m+\u001b[39m [new_text])\n",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_pdf\u001b[39m(pdf_path):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m         pdf_reader \u001b[38;5;241m=\u001b[39m PdfReader(file)\n\u001b[1;32m      4\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/ML_projects/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './pdf_results/'"
     ]
    }
   ],
   "source": [
    "# Step 5: Script for Updating/Retraining\n",
    "def update_and_retrain(new_pdf_path):\n",
    "    new_text = extract_text_from_pdf(new_pdf_path)\n",
    "    # Update the retrieval model's index\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(corpus + [new_text])\n",
    "\n",
    "    # Retrain the generation model on the extended dataset\n",
    "    # Placeholder - replace with actual retraining process\n",
    "    # Ensure to update the dataset and dataloader\n",
    "    # ...\n",
    "\n",
    "    # Save the retrained model\n",
    "    retrained_model_path = os.path.join(output_folder, \"retrained_model\")\n",
    "    generation_model.save_pretrained(retrained_model_path)\n",
    "\n",
    "# Example usage for updating and retraining\n",
    "new_pdf_path = \"./pdf_results/\"\n",
    "update_and_retrain(new_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b17bdf85-2d74-425c-b60d-3bfa55f8247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb main.py        \u001b[34mpdf_results\u001b[m\u001b[m    \u001b[34mpdfs_train\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546545c-6db6-4e65-9cfe-6c9b6a1743e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
